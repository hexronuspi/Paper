{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1355361,"sourceType":"datasetVersion","datasetId":789090},{"sourceId":9026656,"sourceType":"datasetVersion","datasetId":5440264}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Dataset Used: hindi-english-parallel-corpus IITB text data","metadata":{}},{"cell_type":"markdown","source":"Download the Dataset","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv('/kaggle/input/hindi-english-parallel-corpus/hindi_english_parallel.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torchtext\nimport torch\nfrom torchtext.data.utils import get_tokenizer\nfrom collections import Counter\nfrom torchtext.vocab import Vocab\nfrom transformers import AutoTokenizer\nfrom torchtext.utils import download_from_url, extract_archive\nimport io","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"using regular expression library (re), to remove speical chars and extra spaces","metadata":{}},{"cell_type":"code","source":"import re\n\ndef clear_text(sentences):\n    remove_special_chars = re.compile(r'[^A-Za-z\\sअ-हक-ह]')\n    remove_extra_spaces = re.compile(r'\\s+')\n    \n    cleaned_sentences = []\n    for sentence in sentences:\n        if not isinstance(sentence, str):\n            sentence = str(sentence)\n        cleaned_sentence = remove_special_chars.sub('', sentence)\n        cleaned_sentence = remove_extra_spaces.sub(' ', cleaned_sentence).strip()\n        cleaned_sentences.append(cleaned_sentence)\n    \n    return cleaned_sentences\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"performing text cleaning","metadata":{}},{"cell_type":"code","source":"hindi_sentences = df['hindi']\nenglish_sentences = df['english']\n\nhindi_sentences = (clear_text(hindi_sentences.tolist()))\nenglish_sentences = (clear_text(english_sentences.tolist()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(hindi_sentences[:100], english_sentences[:100])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"using the spacy's english tokeniser and ai4bharat/indic-bert for hindi","metadata":{}},{"cell_type":"code","source":"hi_tokenizer = AutoTokenizer.from_pretrained(\"ai4bharat/indic-bert\")\nen_tokenizer = get_tokenizer('spacy', language='en_core_web_sm')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tokenize_hindi_sentences(sentences, tokenizer):\n    return [tokenizer.tokenize(sentence) for sentence in sentences]\n\ndef tokenize_english_sentences(sentences, tokenizer):\n    return [tokenizer(sentence) for sentence in sentences]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"tokenize the sentences and built a token array","metadata":{}},{"cell_type":"code","source":"tokenized_hindi = tokenize_hindi_sentences(hindi_sentences, hi_tokenizer)\ntokenized_english = tokenize_english_sentences(english_sentences, en_tokenizer)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Saving the tokens as it costs a lot on CPU to tokeinize, the file size was around `516.9MiB`.","metadata":{}},{"cell_type":"code","source":"pd.DataFrame({\n    'tokenized_hindi': tokenized_hindi,\n    'tokenized_english': tokenized_english\n}).to_csv('tokenized_sentences.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Hindi: {len(tokenized_hindi)}, English: {len(tokenized_english)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_tok = 5\nprint(f\"Hindi: {tokenized_hindi[:num_tok]}, English: {tokenized_english[:num_tok]}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from torchtext.vocab import build_vocab_from_iterator\nfrom collections import Counter","metadata":{"execution":{"iopub.status.busy":"2024-07-24T16:19:18.609513Z","iopub.execute_input":"2024-07-24T16:19:18.610376Z","iopub.status.idle":"2024-07-24T16:19:21.641983Z","shell.execute_reply.started":"2024-07-24T16:19:18.610329Z","shell.execute_reply":"2024-07-24T16:19:21.640975Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport torchtext\nimport torch\nfrom torchtext.data.utils import get_tokenizer\nfrom collections import Counter\nfrom torchtext.vocab import Vocab\nfrom transformers import AutoTokenizer\nfrom torchtext.utils import download_from_url, extract_archive\nimport io","metadata":{"execution":{"iopub.status.busy":"2024-07-24T16:19:21.643517Z","iopub.execute_input":"2024-07-24T16:19:21.643988Z","iopub.status.idle":"2024-07-24T16:19:21.801068Z","shell.execute_reply.started":"2024-07-24T16:19:21.643960Z","shell.execute_reply":"2024-07-24T16:19:21.800216Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df_token = pd.read_csv('/kaggle/input/token-english-hindi/tokenized_sentences.csv')\n\ntokenized_hindi = df_token['tokenized_hindi'].apply(eval).tolist()\ntokenized_english = df_token['tokenized_english'].apply(eval).tolist()","metadata":{"execution":{"iopub.status.busy":"2024-07-24T16:19:21.802169Z","iopub.execute_input":"2024-07-24T16:19:21.802476Z","iopub.status.idle":"2024-07-24T16:21:21.626364Z","shell.execute_reply.started":"2024-07-24T16:19:21.802440Z","shell.execute_reply":"2024-07-24T16:21:21.625309Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"specials=['<unk>', '<pad>', '<bos>', '<eos>']","metadata":{"execution":{"iopub.status.busy":"2024-07-24T16:21:21.628526Z","iopub.execute_input":"2024-07-24T16:21:21.628862Z","iopub.status.idle":"2024-07-24T16:21:21.633167Z","shell.execute_reply.started":"2024-07-24T16:21:21.628818Z","shell.execute_reply":"2024-07-24T16:21:21.632278Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"def yield_tokens(tokens_list):\n    for tokens in tokens_list:\n        yield tokens","metadata":{"execution":{"iopub.status.busy":"2024-07-24T16:21:21.666213Z","iopub.execute_input":"2024-07-24T16:21:21.666540Z","iopub.status.idle":"2024-07-24T16:21:21.671066Z","shell.execute_reply.started":"2024-07-24T16:21:21.666511Z","shell.execute_reply":"2024-07-24T16:21:21.670185Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def build_vocab_from_tokens(tokens_list, specials):\n    counter = Counter(token for sentence in tokens_list for token in sentence)\n    vocab = build_vocab_from_iterator(yield_tokens(tokens_list), specials=specials)\n    vocab.set_default_index(vocab['<unk>']) \n    return vocab","metadata":{"execution":{"iopub.status.busy":"2024-07-24T16:21:21.672376Z","iopub.execute_input":"2024-07-24T16:21:21.672664Z","iopub.status.idle":"2024-07-24T16:21:21.679648Z","shell.execute_reply.started":"2024-07-24T16:21:21.672634Z","shell.execute_reply":"2024-07-24T16:21:21.678816Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"build a vocab by using the tokens","metadata":{}},{"cell_type":"code","source":"hi_vocab = build_vocab_from_tokens(tokenized_hindi, specials)\nen_vocab = build_vocab_from_tokens(tokenized_english, specials)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T16:21:21.680917Z","iopub.execute_input":"2024-07-24T16:21:21.681266Z","iopub.status.idle":"2024-07-24T16:21:40.002924Z","shell.execute_reply.started":"2024-07-24T16:21:21.681234Z","shell.execute_reply":"2024-07-24T16:21:40.001888Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"print(f\"Hindi Vocabulary Size: {len(hi_vocab)}\")\nprint(f\"English Vocabulary Size: {len(en_vocab)}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-24T16:21:40.004223Z","iopub.execute_input":"2024-07-24T16:21:40.004603Z","iopub.status.idle":"2024-07-24T16:21:40.010338Z","shell.execute_reply.started":"2024-07-24T16:21:40.004569Z","shell.execute_reply":"2024-07-24T16:21:40.009435Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"Hindi Vocabulary Size: 8953\nEnglish Vocabulary Size: 240189\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\ndef tokens_to_tensor(tokens, vocab):\n    return torch.tensor([vocab[token] for token in tokens], dtype=torch.long)\n\ndef data_to_tensors(hindi_tokens_list, english_tokens_list, hi_vocab, en_vocab):\n    data = []\n    for hi_tokens, en_tokens in zip(hindi_tokens_list, english_tokens_list):\n        hi_tensor = tokens_to_tensor(hi_tokens, hi_vocab)\n        en_tensor = tokens_to_tensor(en_tokens, en_vocab)\n        data.append((hi_tensor, en_tensor))\n    return data","metadata":{"execution":{"iopub.status.busy":"2024-07-24T16:21:40.011673Z","iopub.execute_input":"2024-07-24T16:21:40.012023Z","iopub.status.idle":"2024-07-24T16:21:40.020494Z","shell.execute_reply.started":"2024-07-24T16:21:40.011991Z","shell.execute_reply":"2024-07-24T16:21:40.019715Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"convert the voacab to torch.tensors as PyTorch train-loader only works with torch.tensors","metadata":{}},{"cell_type":"code","source":"data = data_to_tensors(tokenized_hindi, tokenized_english, hi_vocab, en_vocab)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T16:21:40.023896Z","iopub.execute_input":"2024-07-24T16:21:40.024157Z","iopub.status.idle":"2024-07-24T16:23:05.190736Z","shell.execute_reply.started":"2024-07-24T16:21:40.024135Z","shell.execute_reply":"2024-07-24T16:23:05.189855Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2024-07-24T16:23:05.191912Z","iopub.execute_input":"2024-07-24T16:23:05.192215Z","iopub.status.idle":"2024-07-24T16:23:05.649374Z","shell.execute_reply.started":"2024-07-24T16:23:05.192185Z","shell.execute_reply":"2024-07-24T16:23:05.648283Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"train test split","metadata":{}},{"cell_type":"code","source":"train_data, temp_data = train_test_split(data, test_size=0.2, random_state=42)\nval_data, test_data = train_test_split(temp_data, test_size=0.5, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T16:23:05.650689Z","iopub.execute_input":"2024-07-24T16:23:05.651012Z","iopub.status.idle":"2024-07-24T16:23:06.369609Z","shell.execute_reply.started":"2024-07-24T16:23:05.650984Z","shell.execute_reply":"2024-07-24T16:23:06.368662Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\n","metadata":{"execution":{"iopub.status.busy":"2024-07-24T16:23:06.371036Z","iopub.execute_input":"2024-07-24T16:23:06.371369Z","iopub.status.idle":"2024-07-24T16:23:06.375871Z","shell.execute_reply.started":"2024-07-24T16:23:06.371341Z","shell.execute_reply":"2024-07-24T16:23:06.374920Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"- **`BATCH_SIZE`**:  \n  Defines the number of samples processed together in one iteration. For example, a `BATCH_SIZE` of `8` means that `8` sequences are processed simultaneously.\n\n- **`PAD_IDX`**:  \n  The index used to pad sequences to ensure they are of equal length. Padding is crucial for processing variable-length sequences in a batch.\n\n- **`BOS_IDX`**:  \n  The index for the **Beginning-Of-Sequence** token, marking the start of a sequence.\n\n- **`EOS_IDX`**:  \n  The index for the **End-Of-Sequence** token, marking the end of a sequence.\n\n**Concept:** Properly setting these hyperparameters ensures that sequences are uniformly processed and aligned, which is critical for training Seq2Seq models.\n","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nBATCH_SIZE = 8\nPAD_IDX = hi_vocab['<pad>']  \nBOS_IDX = hi_vocab['<bos>']\nEOS_IDX = hi_vocab['<eos>']  \n\ndef generate_batch(data_batch):\n    hi_batch, en_batch = [], []\n    for (hi_item, en_item) in data_batch:\n        hi_sequence = torch.cat([torch.tensor([BOS_IDX]), hi_item, torch.tensor([EOS_IDX])], dim=0)\n        en_sequence = torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0)\n        \n        hi_batch.append(hi_sequence)\n        en_batch.append(en_sequence)\n    \n    hi_batch = pad_sequence(hi_batch, padding_value=PAD_IDX)\n    en_batch = pad_sequence(en_batch, padding_value=PAD_IDX)\n    \n    return hi_batch, en_batch\n\ntrain_iter = DataLoader(train_data, batch_size=BATCH_SIZE,\n                        shuffle=True, collate_fn=generate_batch)\n\nvalid_iter = DataLoader(val_data, batch_size=BATCH_SIZE,\n                        shuffle=False, collate_fn=generate_batch)  \n\ntest_iter = DataLoader(test_data, batch_size=BATCH_SIZE,\n                       shuffle=False, collate_fn=generate_batch)  \n\nfor hi_batch, en_batch in train_iter:\n    print(f\"Train batch - Hindi batch shape: {hi_batch.shape}, Eenglish batch shape: {en_batch.shape}\")\n    break\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Theory\n\nIn this implementation, we are building a sequence-to-sequence (Seq2Seq) model with attention mechanism using PyTorch. The model is composed of three main components:\n\n- **Encoder**: Processes input sequences and generates context-aware representations.\n- **Attention**: Computes alignment scores and weights for the encoder outputs.\n- **Decoder**: Generates output sequences based on context from the encoder and attention mechanism.\n","metadata":{}},{"cell_type":"markdown","source":"### Encoder\n- **Embedding Layer**: Converts input tokens into dense vectors.\n- **RNN/GRU/LSTM**: Processes the embedded tokens to capture sequential dependencies and generate hidden states.\n- **Fully Connected Layer**: Maps the final hidden state to the desired output dimension.\n\n**Key Points:**\n\n- The encoder can be bidirectional to capture information from both directions in the sequence.\n- The final hidden states are often concatenated and transformed to match the decoder's hidden state dimension.\n\n### Attention Mechanism\n\nThe Attention Mechanism allows the model to focus on different parts of the input sequence when generating each token in the output sequence. This is crucial for tasks where certain parts of the input are more relevant to specific output tokens.\n\n\n**Key Points:**\n\n- Computes alignment scores between the decoder’s current hidden state and all encoder outputs.\n- These scores are used to compute a weighted sum of the encoder outputs, which represents the context vector for the current decoder step.\n\n\n### Decoder\n\nThe Decoder generates the output sequence using the context vector from the attention mechanism and its own previous outputs. It typically includes:\n\n\n- **Embedding Layer**: Converts output tokens into dense vectors.\n- **RNN/GRU/LSTM**: Processes the combined embedding and context vector to produce hidden states.\n- **Fully Connected Layer**: Maps the final hidden state to the output vocabulary size.\n\n\n**Key Points:**\n\n- Uses the attention context to adjust the generated sequence based on relevant parts of the input.\n\n- Employs teacher forcing during training to improve convergence.\n\nteacher forcing: Teacher Forcing is a training strategy where, during the training phase, the model receives the true output sequence from the training data as input to the decoder at each time step, rather than relying solely on its own predictions from the previous time step.\n\n\n\n### Seq2Seq Model\n\nThe Seq2Seq Model integrates the encoder, decoder, and attention mechanism to perform end-to-end sequence prediction.\n\n**Key Points:**\n\n- The model handles input sequences and generates output sequences with teacher forcing during training.\n\n- Outputs are accumulated over time steps and returned as the final predictions.\n\n\n","metadata":{}},{"cell_type":"code","source":"import random\nfrom typing import Tuple\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch import Tensor\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader\n\nclass Encoder(nn.Module):\n    def __init__(self,\n                 input_dim: int,\n                 emb_dim: int,\n                 enc_hid_dim: int,\n                 dec_hid_dim: int,\n                 dropout: float):\n        super().__init__()\n\n        self.input_dim = input_dim\n        self.emb_dim = emb_dim\n        self.enc_hid_dim = enc_hid_dim\n        self.dec_hid_dim = dec_hid_dim\n        self.dropout = dropout\n\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.rnn = nn.GRU(emb_dim, enc_hid_dim, bidirectional=True)\n        self.fc = nn.Linear(enc_hid_dim * 2, dec_hid_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, src: Tensor) -> Tuple[Tensor]:\n        embedded = self.dropout(self.embedding(src))\n        outputs, hidden = self.rnn(embedded)\n        hidden = torch.tanh(self.fc(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)))\n        return outputs, hidden\n\nclass Attention(nn.Module):\n    def __init__(self,\n                 enc_hid_dim: int,\n                 dec_hid_dim: int,\n                 attn_dim: int):\n        super().__init__()\n\n        self.enc_hid_dim = enc_hid_dim\n        self.dec_hid_dim = dec_hid_dim\n        self.attn_in = (enc_hid_dim * 2) + dec_hid_dim\n        self.attn = nn.Linear(self.attn_in, attn_dim)\n\n    def forward(self,\n                decoder_hidden: Tensor,\n                encoder_outputs: Tensor) -> Tensor:\n        src_len = encoder_outputs.shape[0]\n        repeated_decoder_hidden = decoder_hidden.unsqueeze(1).repeat(1, src_len, 1)\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n        energy = torch.tanh(self.attn(torch.cat((repeated_decoder_hidden, encoder_outputs), dim=2)))\n        attention = torch.sum(energy, dim=2)\n        return F.softmax(attention, dim=1)\n\nclass Decoder(nn.Module):\n    def __init__(self,\n                 output_dim: int,\n                 emb_dim: int,\n                 enc_hid_dim: int,\n                 dec_hid_dim: int,\n                 dropout: float,\n                 attention: nn.Module):\n        super().__init__()\n\n        self.emb_dim = emb_dim\n        self.enc_hid_dim = enc_hid_dim\n        self.dec_hid_dim = dec_hid_dim\n        self.output_dim = output_dim\n        self.dropout = dropout\n        self.attention = attention\n\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.rnn = nn.GRU((enc_hid_dim * 2) + emb_dim, dec_hid_dim)\n        self.out = nn.Linear(self.attention.attn_in + emb_dim, output_dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def _weighted_encoder_rep(self,\n                              decoder_hidden: Tensor,\n                              encoder_outputs: Tensor) -> Tensor:\n        a = self.attention(decoder_hidden, encoder_outputs)\n        a = a.unsqueeze(1)\n        encoder_outputs = encoder_outputs.permute(1, 0, 2)\n        weighted_encoder_rep = torch.bmm(a, encoder_outputs)\n        weighted_encoder_rep = weighted_encoder_rep.permute(1, 0, 2)\n        return weighted_encoder_rep\n\n    def forward(self,\n                input: Tensor,\n                decoder_hidden: Tensor,\n                encoder_outputs: Tensor) -> Tuple[Tensor]:\n        input = input.unsqueeze(0)\n        embedded = self.dropout(self.embedding(input))\n        weighted_encoder_rep = self._weighted_encoder_rep(decoder_hidden, encoder_outputs)\n        rnn_input = torch.cat((embedded, weighted_encoder_rep), dim=2)\n        output, decoder_hidden = self.rnn(rnn_input, decoder_hidden.unsqueeze(0))\n        embedded = embedded.squeeze(0)\n        output = output.squeeze(0)\n        weighted_encoder_rep = weighted_encoder_rep.squeeze(0)\n        output = self.out(torch.cat((output, weighted_encoder_rep, embedded), dim=1))\n        return output, decoder_hidden.squeeze(0)\n\nclass Seq2Seq(nn.Module):\n    def __init__(self,\n                 encoder: nn.Module,\n                 decoder: nn.Module,\n                 device: torch.device):\n        super().__init__()\n\n        self.encoder = encoder\n        self.decoder = decoder\n        self.device = device\n\n    def forward(self,\n                src: Tensor,\n                trg: Tensor,\n                teacher_forcing_ratio: float = 0.5) -> Tensor:\n        batch_size = src.shape[1]\n        max_len = trg.shape[0]\n        trg_vocab_size = self.decoder.output_dim\n        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(self.device)\n        encoder_outputs, hidden = self.encoder(src)\n        output = trg[0, :]\n        for t in range(1, max_len):\n            output, hidden = self.decoder(output, hidden, encoder_outputs)\n            outputs[t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            top1 = output.max(1)[1]\n            output = (trg[t] if teacher_force else top1)\n        return outputs\n\nINPUT_DIM = len(hi_vocab) \nOUTPUT_DIM = len(en_vocab)\nENC_EMB_DIM = 4\nDEC_EMB_DIM = 4\nENC_HID_DIM = 8\nDEC_HID_DIM = 8\nATTN_DIM = 1\nENC_DROPOUT = 0.5\nDEC_DROPOUT = 0.5\n\nenc = Encoder(INPUT_DIM, ENC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, ENC_DROPOUT)\nattn = Attention(ENC_HID_DIM, DEC_HID_DIM, ATTN_DIM)\ndec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, ENC_HID_DIM, DEC_HID_DIM, DEC_DROPOUT, attn)\n\nmodel = Seq2Seq(enc, dec, device).to(device)\n\ndef init_weights(m: nn.Module):\n    for name, param in m.named_parameters():\n        if 'weight' in name:\n            nn.init.normal_(param.data, mean=0, std=0.01)\n        else:\n            nn.init.constant_(param.data, 0)\n\nmodel.apply(init_weights)\n\noptimizer = optim.Adam(model.parameters())\n\ndef count_parameters(model: nn.Module):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-24T16:23:06.640312Z","iopub.execute_input":"2024-07-24T16:23:06.640633Z","iopub.status.idle":"2024-07-24T16:23:07.518043Z","shell.execute_reply.started":"2024-07-24T16:23:06.640606Z","shell.execute_reply":"2024-07-24T16:23:07.517089Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"The model has 15,689,815 trainable parameters\n","output_type":"stream"}]},{"cell_type":"code","source":"PAD_IDX = en_vocab['<pad>']\nBOS_IDX = en_vocab['<bos>']\nEOS_IDX = en_vocab['<eos>']\n\ncriterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)","metadata":{"execution":{"iopub.status.busy":"2024-07-24T16:23:07.519080Z","iopub.execute_input":"2024-07-24T16:23:07.519529Z","iopub.status.idle":"2024-07-24T16:23:07.524388Z","shell.execute_reply.started":"2024-07-24T16:23:07.519502Z","shell.execute_reply":"2024-07-24T16:23:07.523382Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import math\nimport time\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n\ndef train(model: nn.Module,\n          iterator: torch.utils.data.DataLoader,\n          optimizer: optim.Optimizer,\n          criterion: nn.Module,\n          clip: float):\n\n    model.train()\n\n    epoch_loss = 0\n\n    for _, (src, trg) in enumerate(tqdm(iterator, desc='Training')):\n        src, trg = src.to(device), trg.to(device)\n        optimizer.zero_grad()\n        output = model(src, trg)\n        output = output[1:].view(-1, output.shape[-1])\n        trg = trg[1:].view(-1)\n        loss = criterion(output, trg)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        epoch_loss += loss.item()\n\n    return epoch_loss / len(iterator)\n\n\ndef evaluate(model: nn.Module,\n             iterator: torch.utils.data.DataLoader,\n             criterion: nn.Module):\n\n    model.eval()\n    epoch_loss = 0\n\n    with torch.no_grad():\n\n        for _, (src, trg) in enumerate(tqdm(iterator, desc='Evaluating')):\n            src, trg = src.to(device), trg.to(device)\n            output = model(src, trg, 0) \n            output = output[1:].view(-1, output.shape[-1])\n            trg = trg[1:].view(-1)\n            loss = criterion(output, trg)\n            epoch_loss += loss.item()\n\n    return epoch_loss / len(iterator)\n\n\ndef epoch_time(start_time: int,\n               end_time: int):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time / 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs\n\n\nN_EPOCHS = 10\nCLIP = 1\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n\n    start_time = time.time()\n    train_loss = train(model, train_iter, optimizer, criterion, CLIP)\n    valid_loss = evaluate(model, valid_iter, criterion)\n    end_time = time.time()\n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n\n    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n\ntest_loss = evaluate(model, test_iter, criterion)\n\nprint(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')\n","metadata":{"execution":{"iopub.status.busy":"2024-07-24T16:23:07.525724Z","iopub.execute_input":"2024-07-24T16:23:07.526009Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stderr","text":"Training:   2%|▏         | 3291/156184 [21:57<19:08:14,  2.22it/s]","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}